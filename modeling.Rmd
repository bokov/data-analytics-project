## Problem Set 2 - Let's Get Logistical

```{r}

# Load R Data
analytics.table <- readRDS("data/analytics.rdata")

# First, Partition The Data
data.train.indices <- caret::createDataPartition(y = analytics.table[, dependent], p = partition, list = FALSE)
data.train <- analytics.table[data.train.indices, ]
data.valid <- analytics.table[-data.train.indices, ]

# Validate All Data Is In Train/Valid Partitions
stopifnot(nrow(analytics.table) == nrow(data.train) + nrow(data.valid))

# Output n Rows For Data Set & Partitions
message(nrow(analytics.table), " Rows in Data")
message(nrow(data.train), " Rows in Training Set")
message(nrow(data.valid), " Rows in Validation Set")

# How Well Does Just The LACE Sum Do?
formula <- as.formula("readmit_30 ~ LACE_sum")
model.readmit30.lacesum <- glm(formula, data = data.train, family = "binomial")
print(summary(model.readmit30.lacesum ))

# How About The Individual L, A, C, & E?
formula <- as.formula("readmit_30 ~ L + A + C + E")
model.readmit30.l_a_c_e <- glm(formula, data = data.train, family = "binomial")
print(summary(model.readmit30.l_a_c_e))

# What Model Do We Get When We Do Forward-Backward Selection of Covariates?
formula <- as.formula("readmit_30 ~ L + A + C + E + LACE_sum")
model.readmit30.l_a_c_e_lacesum <- glm(formula, data = data.train, family = "binomial")
step(model.readmit30.l_a_c_e_lacesum)

``` 
# Problem Set 2 â€“ Model Analyses

The Akaike Information Criteria (AIC) is lowest for the individual L, A, C, & E Model (AIC = 17140).

```{r}

# Predict On Validation Set
model.readmit30.l_a_c_e.probabilities <- predict(model.readmit30.l_a_c_e, data.valid, type = "response")

# Insert Predicted Probabilities As Column Into Validation Set
data.valid <- data.frame(data.valid, pprobs = model.readmit30.l_a_c_e.probabilities)

# Plot Histogram of Predicted Probabilities
hist(model.readmit30.l_a_c_e.probabilities)

# Find Best Threshold By Measure Tradeoff
# Valid Measures Are Any Two
# > tpr
# > tnr
# > fpr
# > fnr
# > sensitivity
# > specificity
# bestThreshold <- function (model, data, dependent, m1 = "sensitivity", m2 = "specificity", by = 0.05) {
# 	statistics <- list(
# 		"n" = vector(),
# 		"tpr" = vector(),
# 		"fpr" = vector(),
# 		"tnr" = vector(),
# 		"fnr" = vector(),
# 		"ppv" = vector(),
# 		"npv" = vector(),
# 		"sensitivity" = vector(),
# 		"specificity" = vector(),
# 		"accuracy" = vector()
# 	)
# 	m1 <- m1
# 	m2 <- quote(m2)
# 	report <- list()
# 	thresholds <- seq(0.0, 1.0, by = by)
# 	truth <- data[, dependent]
# 	# Iterate Through Thresholds & Find Best Threshold
# 	for (threshold in thresholds) {
# 		probabilities <- predict(model, data, type = "response")
# 		truth <- data[, dependent]
# 		predict <- ifelse(probabilities < threshold, 0, 1)
# 		u <- union(truth, predict)
# 		predictions <- table(factor(truth, u), factor(predict, u))
# 	 	n <- sum(predictions)
# 	 	tp <- predictions[1, 1]
# 	 	tn <- predictions[2, 2]
# 	 	fp <- predictions[1, 2]
# 	 	fn <- predictions[2, 1]
# 	 	positive <- tp + tn
# 	 	negative <- fp + fn
# 	 	# Generate Statistics
# 	 	statistics["n"] <- n
# 		statistics["tp"] <- append(statistics["tp"], tp, length(statistics["tp"]))
# 		statistics["tn"] <- append(statistics["tn"], tn, length(statistics["tn"]))
# 		statistics["fp"] <- append(statistics["fp"], fp, length(statistics["fp"]))
# 		statistics["fn"] <- append(statistics["fn"], fn, length(statistics["fn"]))
# 		statistics["positive"] <- append(statistics["positive"], tp + tn, length(statistics["positive"]))
# 		statistics["negative"] <- append(statistics["negative"], fp + fn, length(statistics["negative"]))
# 		statistics["accuracy"] <- append(statistics["accuracy"], positive / n, length(statistics["accuracy"]))
# 		statistics["ppv"] <- append(statistics["ppv"], tp / (tp + fp), length(statistics["ppv"]))
# 		statistics["npv"] <- append(statistics["npv"], tn / (tn + fn), length(statistics["npv"]))
# 		statistics["tpr"] <- append(statistics["tpr"], tp / positive, length(statistics["tpr"]))
# 		statistics["tnr"] <- append(statistics["tnr"], tn / negative, length(statistics["tnr"]))
# 		statistics["fpr"] <- append(statistics["fpr"], fp / negative, length(statistics["fpr"]))
# 		statistics["fnr"] <- append(statistics["fnr"], fn / positive, length(statistics["fnr"]))
# 		statistics["sensitivity"] <- append(statistics["sensitivity"], tp / positive, length(statistics["sensitivity"]))
# 		statistics["specificity"] <- append(statistics["specificity"], tn / negative, length(statistics["specificity"]))
# 	}
# 	report["index"] <- which.max(statistics[[m1]] + statistics[[m2]])
	# report["threshold"] <- thresholds[report["index"]]
	# report["accuracy"] <- statistics["accuracy"][report["index"]]
	# report[[m1]] <- statistics[[m1]][report["index"]]
	# report[[m2]] <- statistics[[m2]][report["index"]]
	# report
	# print()
# }

# optimal <- bestThreshold(model.readmit30.l_a_c_e, data.valid, "readmit_30")

# message("Threshold: ", optimal["threshold"])
# message("Accuracy: ", optimal["accuracy"])
# message("Sensitivity: ", optimal["sensitivity"])
# message("Specificity: ", optimal["specificity"])
# message("Positive: ", optimal["positive"])
# message("Negative: ", optimal["negative"])

```