---
title: "Project - Data Analytics – Modeling"
author: "Cordier B., Hee D., Swan R."
date: "6/21/2017"
output: html_document
---

## Problem Set 2 - Let's Get Logistical

```{r}

    set.seed(111)

    # Load R Data
    analytics.table <- readRDS("data/analytics.rdata")

    # Final n Rows Check
    message("n Rows: ", dim(analytics.table)[1])
    
    # First, Partition The Data
    data.train.indices <- caret::createDataPartition(y = analytics.table[, dependent], p = partition, list = FALSE)
    data.train <- analytics.table[data.train.indices, ]
    data.valid <- analytics.table[-data.train.indices, ]

    # Validate All Data Is In Train/Valid Partitions
    stopifnot(nrow(analytics.table) == nrow(data.train) + nrow(data.valid))

    # Output n Rows For Data Set & Partitions
    message(nrow(analytics.table), " Rows in Data")
    message(nrow(data.train), " Rows in Training Set")
    message(nrow(data.valid), " Rows in Validation Set")

    # How Well Does Just The LACE Sum Do?
    formula <- as.formula("readmit_30 ~ LACE_sum")
    model.readmit30.lacesum <- glm(formula, data = data.train, family = "binomial")
    print(summary(model.readmit30.lacesum))

    # How About The Individual L, A, C, & E?
    formula <- as.formula("readmit_30 ~ L + A + C + E")
    model.readmit30.l_a_c_e <- glm(formula, data = data.train, family = "binomial")
    print(summary(model.readmit30.l_a_c_e))

    # How About The Individual L, A, C, & E + LACE Sum?
    formula <- as.formula("readmit_30 ~ L + A + C + E + LACE_sum")
    model.readmit30.l_a_c_e_lacesum <- glm(formula, data = data.train, family = "binomial")
    print(summary(model.readmit30.l_a_cwc_e))

    # How About The Individual L, A, C_weight, & E?
    formula <- as.formula("readmit_30 ~ L + A + C_weight + E")
    model.readmit30.l_a_cw_e <- glm(formula, data = data.train, family = "binomial")
    print(summary(model.readmit30.l_a_cw_e))

    # How About The Individual L, A, C_weight, & E + LAC_wE Sum?
    formula <- as.formula("readmit_30 ~ L + A + C_weight + E + LACE_sum_C_weight")
    model.readmit30.l_a_cw_e_lacesumw <- glm(formula, data = data.train, family = "binomial")
    print(summary(model.readmit30.l_a_cwc_e))

    # How About The Individual L, A, C_weight_cut, & E?
    formula <- as.formula("readmit_30 ~ L + A + C_weight_cut + E")
    model.readmit30.l_a_cwc_e <- glm(formula, data = data.train, family = "binomial")
    print(summary(model.readmit30.l_a_cwc_e))

    # How About The Individual L, A, C_weight_cut, & E + LAC_wcE Sum?
    formula <- as.formula("readmit_30 ~ L + A + C_weight_cut + E + LACE_sum_C_weight_cut")
    model.readmit30.l_a_cw_e_lacesumwc <- glm(formula, data = data.train, family = "binomial")
    print(summary(model.readmit30.l_a_cw_e_lacesumwc))

``` 
Seems our weighting scheme is a case study on one developed by a person lacking in domain expertise...whoops! We can stick with the standard `model.readmit30.l_a_c_e` model, which has the lowest Akaike information criterion score (AIC = 17152). We can compare it to the simple LACE Sum model, which has a higher AIC (18508).

# Problem Set 2 – Model Analyses

```{r, fig.width = 6, fig.height = 6}

    # Predict On Validation Set
    model.readmit30.l_a_c_e.probabilities <- predict(model.readmit30.l_a_c_e, data.valid, type = "response")

    # Insert Predicted Probabilities As Column Into Validation Set
    data.valid <- data.frame(data.valid, pprobs = model.readmit30.l_a_c_e.probabilities)

    # Plot Histogram of Predicted Probabilities
    hist(model.readmit30.l_a_c_e.probabilities)

```

Long tail; most patients are predicted as having a low probability of readmission within 30 days. There is a long tail of patients that have a high predicted probability. 

## First A Function Definition Then Some Questions
```{r}

    # Find Best Probability Threshold By Measure Tradeoff
    # Valid Measures Are Any Two
    # > tpr
    # > tnr
    # > fpr
    # > fnr
    # > sensitivity
    # > specificity
    maximizeByThreshold <- function (model, data, dependent, m1 = "sensitivity", m2 = "specificity", by = 0.01) {
        
        truth <- data[, dependent]
        thresholds <- seq(0.0, 1.0, by = by)
        nThresholds <- length(thresholds)
        l <- rep(1, nThresholds)
        n <- dim(data)[1]
        statistics <- list(
            "tpr" = l,
            "fpr" = l,
            "tnr" = l,
            "fnr" = l,
            "ppv" = l,
            "npv" = l,
            "sensitivity" = l,
            "specificity" =l,
            "accuracy" = l,
            "positive" = l,
            "negative" = l,
            "correct" = l,
            "incorrect" = l
        )
        report <- list()
        # Iterate Through Thresholds & Find Best Threshold
        for (i in 1:nThresholds) {
            threshold <- thresholds[i]
            probabilities <- predict(model, data, type = "response")
            predict <- ifelse(probabilities < threshold, 0, 1)
            u <- union(truth, predict)
            predictions <- table(factor(predict, u), factor(truth, u))
            tp <- predictions[1, 1]
            tn <- predictions[2, 2]
            fp <- predictions[1, 2]
            fn <- predictions[2, 1]
            positive <- tp + tn
            negative <- fp + fn
            # Generate Statistics
            statistics$thresholds <- thresholds
            statistics$tp[i] <- tp
            statistics$tn[i] <- tn
            statistics$fp[i] <- fp
            statistics$fn[i] <- fn
            statistics$positive[i] <- (tp + fn)
            statistics$negative[i] <- (tn + fp)
            statistics$correct[i] <- (tp + tn)
            statistics$incorrect[i] <- (fp + fn)
            statistics$accuracy[i] <- ((tp + tn) / n)
            statistics$ppv[i] <- (tp / (tp + fp))
            statistics$npv[i] <- (tn / (tn + fn))
            statistics$tpr[i] <- (tp / (tp + fn))
            statistics$tnr[i] <- (tn / (tn + fp))
            statistics$fpr[i] <- (fp / (fp + tn))
            statistics$fnr[i] <- (fn / (fn + tp))
            statistics$sensitivity[i] <- (tp / (tp + fn))
            statistics$specificity[i] <- (tn / (tn + fp))
        }
        report["index"] <- which.max(statistics[[m1]] + statistics[[m2]])
        report["threshold"] <- thresholds[report[["index"]]]
        report["accuracy"] <- statistics[["accuracy"]][report[["index"]]]
        report["correct"] <- statistics[["correct"]][report[["index"]]]
        report["incorrect"] <- statistics[["incorrect"]][report[["index"]]]
        report[[m1]] <- statistics[[m1]][report[["index"]]]
        report[[m2]] <- statistics[[m2]][report[["index"]]]
        report["n"] <- n
        result <- list(
            report = report,
            stats = statistics
        )
        result
    }

```

## How Does The LACE Sum Perform With A Classification Of LACEsum > 10?

```{r}

    # Set Truth, n, & Predict At Threshold > 10
    truth <- data.valid[, dependent]
    n <- dim(data.valid)[1]
    predict <- ifelse(data.valid$LACE_sum > 10, 1, 0)

    # Compute Confusion Matrix
    u <- union(truth, predict)
    predictions <- table(factor(predict, u), factor(truth, u))
    tp <- predictions[1, 1]
    tn <- predictions[2, 2]
    fp <- predictions[1, 2]
    fn <- predictions[2, 1]

    # Compute Statistics
    statistics <- list()
    statistics$n <- n
    statistics$tp <- tp
    statistics$tn <- tn
    statistics$fp <- fp
    statistics$fn <- fn
    statistics$positive <- (tp + fn)
    statistics$negative <- (tn + fp)
    statistics$correct <- (tp + tn)
    statistics$incorrect <- (fp + fn)
    statistics$accuracy <- ((tp + tn) / n)
    statistics$ppv <- (tp / (tp + fp))
    statistics$npv <- (tn / (tn + fn))
    statistics$tpr <- (tp / (tp + fn))
    statistics$tnr <- (tn / (tn + fp))
    statistics$fpr <- (fp / (fp + tn))
    statistics$fnr <- (fn / (fn + tp))
    statistics$sensitivity <- (tp / (tp + fn))
    statistics$specificity <- (tn / (tn + fp))

    # Statistics By LACE Sum Threshold > 10
    message(
        "Correct: ", statistics$correct, "\n",
        "Incorrect: ", statistics$incorrect, "\n",
        "Accuracy: ", statistics$accuracy, "\n",
        "Sensitivity: ", statistics$sensitivity, "\n",
        "Specificity: ", statistics$specificity, "\n",
        "FPR: ", statistics$fpr, "\n",
        "FNR: ", statistics$fnr, "\n",
        "n: ", statistics$n
    )

```

Good accuracy (72.06%)...but low sensitivity (32.83%), the accuracy is largely coming from the specificity (91.91%) and, I would assert that we may want to emphasize sensitivity over specificity in our context (i.e. flagging patients as being at high risk for readmission within 30 days, with the aim of stabilizing high risk patients healthcare and pursuing the longterm cost effectiveness of their care through better health – albeit, while still being aware that specificity matters in terms of reducing cost and utilization).

## For The LACE Sum Model, What Probability Threshold Maximizes The Sum of Sensitivity + Specificity?
#### i.e. Assuming Utilization & Detection Operate On A Uniform Spectrum, How Can We Maximize Detection While Minimizing Utilization?

```{r, fig.width = 6, fig.height = 6}

    # Get Optimal Probability Threshold
    optimal.lacesum <- bestThreshold(model.readmit30.lacesum, data.valid, "readmit_30", by = 0.001)

    # Predict On Validation Set
    model.readmit30.lacesum.probabilities <- predict(model.readmit30.lacesum, data.valid, type = "response")

    # Compute ROC Curve
    roc.prediction <- ROCR::prediction(model.readmit30.lacesum.probabilities, data.valid[, dependent])
    roc.performance <- ROCR::performance(roc.prediction, measure = "tpr", x.measure = "fpr")

    # Compute AUC
    roc.auc <- ROCR::performance(roc.prediction, measure = "auc")@y.values

    # Plot ROC
    plot(roc.performance, measure = "tpr", x.measure = "fpr")
    abline(coef = c(0, 1))
    title(paste("ROC For readmit30 ~ LACE Sum Model (Response) (AUC = ", round(as.numeric(roc.auc), 4), ")", sep = ""))


    # Output By Optimal Sensitivity + Specificity
    message(
        "Threshold: ", optimal.lacesum$report["threshold"], "\n",
        "Correct: ", optimal.lacesum$report["correct"], "\n",
        "Incorrect: ", optimal.lacesum$report["incorrect"], "\n",
        "Accuracy: ", optimal.lacesum$report["accuracy"], "\n",
        "Sensitivity: ", optimal.lacesum$report["sensitivity"], "\n",
        "Specificity: ", optimal.lacesum$report["specificity"], "\n",
        "FPR: ", optimal.lacesum$report["fpr"], "\n",
        "FNR: ", optimal.lacesum$report["fnr"], "\n",
        "n: ", optimal.lacesum$report["n"]
    )

```

AUC is pretty impressive considering the simplicity of the `readmit_30 ~ LACEsum` model. It looks like we can improve sensitivity substantially (58.79%) with a hit to specificity (72.45%) and accuracy (61.25%) if we optimize our risk stratification (i.e. high risk or low risk) by a tuned probability threshold of probability of 15.1% risk for readmission within 30 days.

## For The LACE Sum Model, What Threshold Maximizes The Sum of Accuracy + Sensitivity?
#### i.e. How Can We Maximize Overall Accuracy With A Preference Toward Maximizing Detection?

```{r}

    # Output By Optimal Accuracy + Sensitivity
    idx <- which.max(optimal.lacesum$stats[["accuracy"]] + optimal.lacesum$stats[["sensitivity"]])
    message(
        "Threshold: ", optimal.lacesum$stats[["thresholds"]][idx], "\n",
        "Correct: ", optimal.lacesum$stats[["correct"]][idx], "\n",
        "Incorrect: ", optimal.lacesum$stats[["incorrect"]][idx], "\n",
        "Accuracy: ", optimal.lacesum$stats[["accuracy"]][idx], "\n",
        "Sensitivity: ", optimal.lacesum$stats[["sensitivity"]][idx], "\n",
        "Specificity: ", optimal.lacesum$stats[["specificity"]][idx], "\n",
        "FPR: ", optimal.lacesum$stats[["fpr"]][idx], "\n",
        "FNR: ", optimal.lacesum$stats[["fnr"]][idx], "\n",
        "n: ", optimal.lacesum$report["n"]
    )

```

If we maximize our accuracy and sensitivity, we get a much more sensitive model (82.18% sensitivity), however it appears to have low accuracy (44.51%) – without going into too much detail, it seems a threshold of 8.9% probability for readmission within 30 days is likely to flag most patients (high false positive rate) and have a low specificity (45.41%) but get a large proportion of the patients who are on course to get readmitted within 30 days. Again, pretty impressive for a single score. 

## For The LACE Sum Model, What Threshold Maximizes The Sum of Accuracy + Specificity?
#### i.e. How Can We Maximize Overall Accuracy With A Preference Toward Minimizing Utilization?
```{r}

    # Output By Optimal Accuracy + Specificity
    idx <- which.max(optimal.lacesum$stats[["accuracy"]] + optimal.lacesum$stats[["specificity"]])
    message(
        "Threshold: ", optimal.lacesum$stats[["thresholds"]][idx], "\n",
        "Correct: ", optimal.lacesum$stats[["correct"]][idx], "\n",
        "Incorrect: ", optimal.lacesum$stats[["incorrect"]][idx], "\n",
        "Accuracy: ", optimal.lacesum$stats[["accuracy"]][idx], "\n",
        "Sensitivity: ", optimal.lacesum$stats[["sensitivity"]][idx], "\n",
        "Specificity: ", optimal.lacesum$stats[["specificity"]][idx], "\n",
        "FPR: ", optimal.lacesum$stats[["fpr"]][idx], "\n",
        "FNR: ", optimal.lacesum$stats[["fnr"]][idx], "\n",
        "n: ", optimal.lacesum$report["n"]
    )

```

And if we wanted to clamp down on utilization in the short term, well, this model would be one way to do it. It would probably come back to haunt us – with 99.8% specificity and only 3.8% sensitivity, this is effectively an argument to not prioritize any but the most severe cases and a compelling example of where accuracy (which is pretty good here, at 73.96%) may not be the metric to prioritize. Cold blooded, with a probability threshold of greater than or equal to 51.3% chance of readmission within 30 days being required to be considered high risk.

## For The L + A + C + E Model, What Threshold Maximizes The Sum of Sensitivity + Specificity?
#### i.e. Assuming Utilization & Detection Operate On A Uniform Spectrum, How Can We Maximize Detection While Minimizing Utilization?
```{r, fig.width = 6, fig.height = 6}

    # Get Optimal From model.readmit30.l_a_c_e
    optimal.l_a_c_e <- bestThreshold(model.readmit30.l_a_c_e, data.valid, "readmit_30", by = 0.001)

    # Compute ROC Curve
    roc.prediction <- ROCR::prediction(model.readmit30.l_a_c_e.probabilities, data.valid[, dependent])
    roc.performance <- ROCR::performance(roc.prediction, measure = "tpr", x.measure = "fpr")

    # Compute AUC
    roc.auc <- ROCR::performance(roc.prediction, measure = "auc")@y.values

    plot(roc.performance, measure = "tpr", x.measure = "fpr")
    abline(coef = c(0, 1))
    title(paste("ROC For readmit30 ~ L + A + C + E Model (AUC = ", round(as.numeric(roc.auc), 4), ")", sep = ""))

    # Output By Optimal Sum of Sensitivity And Specificity
    message(
        "Threshold: ", optimal.l_a_c_e$report["threshold"], "\n",
        "Correct: ", optimal.l_a_c_e$report["correct"], "\n",
        "Incorrect: ", optimal.l_a_c_e$report["incorrect"], "\n",
        "Accuracy: ", optimal.l_a_c_e$report["accuracy"], "\n",
        "Sensitivity: ", optimal.l_a_c_e$report["sensitivity"], "\n",
        "Specificity: ", optimal.l_a_c_e$report["specificity"], "\n",
        "FPR: ", optimal.l_a_c_e$report["fpr"], "\n",
        "FNR: ", optimal.l_a_c_e$report["fnr"], "\n",
        "n: ", optimal.l_a_c_e$report["n"]
    )

```

To the `readmit30.l_a_c_e` (separate L + A + C + E covariates) model. The AUC here (AUC = 0.7711) is improved over the simple LACE sum model (AUC = 0.7159) – good news. The 10 added comorbidities are probably a contributing factor here, in addition the lack of information loss from summing of the L + A + C + E into the LACE sum. 

To maximize the sum of sensitivity and specificity, the optimal threshold was found to be a probability of readmission within 30 days of 14.7%. While the accuracy is not great here (65.89%), it is better than the LACE sum model and does exhibit a decent balance between sensitivity (62.12%) and specificity (78.15%). Can we create a model that is both accurate and sensitive though?

## For The L + A + C + E Model, What Threshold Maximizes The Sum of Accuracy + Sensitivity?
#### i.e. How Can We Maximize Overall Accuracy With A Preference Toward Maximizing Detection?
```{r}

    # Output By Optimal Accuracy + Sensitivity
    idx <- which.max(optimal.l_a_c_e$stats[["accuracy"]] + optimal.l_a_c_e$stats[["sensitivity"]])
    message(
        "Threshold: ", optimal.l_a_c_e$stats[["thresholds"]][idx], "\n",
        "Correct: ", optimal.l_a_c_e$stats[["correct"]][idx], "\n",
        "Incorrect: ", optimal.l_a_c_e$stats[["incorrect"]][idx], "\n",
        "Accuracy: ", optimal.l_a_c_e$stats[["accuracy"]][idx], "\n",
        "Sensitivity: ", optimal.l_a_c_e$stats[["sensitivity"]][idx], "\n",
        "Specificity: ", optimal.l_a_c_e$stats[["specificity"]][idx], "\n",
        "FPR: ", optimal.l_a_c_e$stats[["fpr"]][idx], "\n",
        "FNR: ", optimal.l_a_c_e$stats[["fnr"]][idx], "\n",
        "n: ", optimal.l_a_c_e$report["n"]
    )

```

This is pretty good. Here I aimed to find the probability threshold that maximized the sum of accuracy + sensitivity (9.8% probability of readmission within 30 days). While the accuracy is still not great (marginally better than a coin toss), the sensitivity of the model is not good (83.36%) and the specificity is still existent (54.32%). This model would likely be the best of the ones outlined in this document in terms of real implementation, although it potentiall could be tuned further. 

## For The L + A + C + E Model, What Threshold Maximizes The Sum of Accuracy + Specificity?
#### i.e. How Can We Maximize Overall Accuracy With A Preference Toward Minimizing Utilization?
```{r}

    # Output By Optimal Accuracy + Specificity
    idx <- which.max(optimal.l_a_c_e$stats[["accuracy"]] + optimal.l_a_c_e$stats[["specificity"]])
    message(
        "Threshold: ", optimal.l_a_c_e$stats[["thresholds"]][idx], "\n",
        "Correct: ", optimal.l_a_c_e$stats[["correct"]][idx], "\n",
        "Incorrect: ", optimal.l_a_c_e$stats[["incorrect"]][idx], "\n",
        "Accuracy: ", optimal.l_a_c_e$stats[["accuracy"]][idx], "\n",
        "Sensitivity: ", optimal.l_a_c_e$stats[["sensitivity"]][idx], "\n",
        "Specificity: ", optimal.l_a_c_e$stats[["specificity"]][idx], "\n",
        "FPR: ", optimal.l_a_c_e$stats[["fpr"]][idx], "\n",
        "FNR: ", optimal.l_a_c_e$stats[["fnr"]][idx], "\n",
        "n: ", optimal.l_a_c_e$report["n"]
    )

```

And the final configuration for this model, which maximizes the sum of accuracy + specificity. As noted previously, this is the cold blooded breed of model and this variant considers high risk patients to be those with a probability greater than or equal to 64.6% of readmission within 30 days, although at least this one gets four times the sensitivity of the comparable LACE sum model (15.02%). Accuracy is fairly high (75.18%) and specificity is very good (99.4%). 

## Revisiting The Histogram

```{r, fig.width = 6, fig.height = 6}

    # Plot Histogram of Predicted Probabilities
    hist(model.readmit30.l_a_c_e.probabilities)

```

If we look at the histogram, the model which maximizes the sum of sensitivity and specificity, with a threshold probability of 14.7% makes sense. There is a clear difference in the volume of patients below ~15% probability of readmission within 30 days and those above that threshold. There also appear to be a good number of true readmits within 30 days in the second largest bar between 10 - 15%. The probability threshold that maximizes the sum of sensitivity and accuracy (9.8%) classifies those in this bucket as high risk and yields an increase of about 21% sensitivity from doing so at a cost of decreasing specificity by 24%. 